

# ğŸš€ LATAM Airlines DevSecOps/SRE Challenge

## âœ¨ DescripciÃ³n del Proyecto
Este proyecto implementa un sistema en la nube para ingestar, almacenar y exponer datos a travÃ©s de una API HTTP para que puedan ser consumidos por terceros, utilizando infraestructura como cÃ³digo (IaC) con Terraform y flujos de CI/CD con GitHub Actions. AdemÃ¡s, se incorporaron pruebas de integraciÃ³n, carga, monitoreo y propuestas de alertas para garantizar la resiliencia y escalabilidad del sistema.

---
## ğŸ“‚ Parte 0: Estructura del Proyecto

```plaintext
ğŸ“¦ latam-challenge/
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ ğŸ“„ workflows/
â”‚       â””â”€â”€ ğŸ“„ ci-cd-pipeline.yml
â”œâ”€â”€ ğŸ“‚ terraform/
â”‚   â”œâ”€â”€ ğŸ“„ main.tf
â”‚   â”œâ”€â”€ ğŸ“„ variables.tf
â”‚   â”œâ”€â”€ ğŸ“„ outputs.tf
â”‚   â””â”€â”€ ğŸ“„ provider.tf
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â”œâ”€â”€ ğŸ“„ main.py
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt
â”‚   â””â”€â”€ ğŸ“‚ tests/
â”‚       â”œâ”€â”€ ğŸ“„ test_api.py
â”‚       â””â”€â”€ ğŸ“„ test_integration.py
â”œâ”€â”€ ğŸ“‚ load_tests/
â”‚   â””â”€â”€ ğŸ“„ locustfile.py
â”œâ”€â”€ ğŸ“‚ docs/
â”‚   â””â”€â”€ ğŸ“„ Arquitectura-API-Latam.png
â”œâ”€â”€ ğŸ“„ Dockerfile
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ README.md
â””â”€â”€ ğŸ“„ load-challenge.py


## ğŸ—ï¸ Parte 1: Infraestructura e IaC

### **1. Identificar la infraestructura necesaria para ingestar, almacenar y exponer datos**

#### **1.1 Ingesta, almacenamiento y exposiciÃ³n de datos**

##### **1.1.a Utilizar el esquema Pub/Sub para ingesta de datos**
El sistema implementa un esquema de mensajerÃ­a asÃ­ncrona utilizando **Google Cloud Pub/Sub** para garantizar la ingesta eficiente de datos. La arquitectura incluye:

- **TÃ³pico:** `datos-topic`
  - Este tÃ³pico recibe los mensajes publicados por el sistema o usuarios externos.
  - Ejemplo de mensaje publicado:
    ```json
    {
      "id": "123",
      "contenido": "Este es un mensaje de prueba",
      "timestamp": "2025-01-14T10:00:00Z"
    }
    ```

- **SuscripciÃ³n:** `datos-subscription`
  - Procesa los mensajes del tÃ³pico y envÃ­a los datos para su almacenamiento en BigQuery.

**Ventajas del esquema Pub/Sub:**
- Desacopla la producciÃ³n y el consumo de datos, lo que facilita la escalabilidad.
- Proporciona resiliencia ante fallos temporales.
- Admite altos volÃºmenes de datos de manera eficiente.

##### **1.1.b Base de datos para el almacenamiento enfocado en analÃ­tica de datos**
Se utiliza **Google BigQuery** como base de datos principal para almacenar los datos. 

- **Dataset:** `latam_dataset`
- **Tabla:** `datos`
  - Esquema definido:
    ```json
    [
      {
        "name": "id",
        "type": "STRING",
        "mode": "REQUIRED",
        "description": "Identificador Ãºnico del dato"
      },
      {
        "name": "contenido",
        "type": "STRING",
        "mode": "NULLABLE",
        "description": "Contenido del dato"
      },
      {
        "name": "timestamp",
        "type": "TIMESTAMP",
        "mode": "REQUIRED",
        "description": "Marca de tiempo del registro"
      }
    ]
    ```

**Beneficios de BigQuery:**
- Es ideal para tareas analÃ­ticas debido a su alta velocidad de consulta.
- Escalabilidad automÃ¡tica para grandes volÃºmenes de datos.
- FÃ¡cil integraciÃ³n con otros servicios de GCP.

##### **1.1.c Endpoint HTTP para servir parte de los datos almacenados**
Se implementÃ³ una API utilizando **FastAPI** desplegada en **Google Cloud Run**. Esta API expone los datos almacenados en BigQuery y ofrece dos endpoints principales:

- **Endpoint `/health`:** 
  - Devuelve el estado del sistema.
  - Ejemplo de respuesta:
    ```json
    {
      "status": "ok"
    }
    ```

- **Endpoint `/datos`:**
  - Devuelve los Ãºltimos 10 registros almacenados en BigQuery.
  - Ejemplo de respuesta:
    ```json
    {
      "datos": [
        {
          "id": "789",
          "contenido": "Prueba automÃ¡tica",
          "timestamp": "2024-01-14T14:00:00+00:00"
        }
      ]
    }
    ```

---

#### **1.2 Deployar infraestructura mediante Terraform**

Se utilizÃ³ **Terraform** para automatizar la creaciÃ³n y gestiÃ³n de la infraestructura en Google Cloud Platform. Los recursos desplegados incluyen:

- **Google Cloud Pub/Sub:**
  - TÃ³pico: `datos-topic`
  - SuscripciÃ³n: `datos-subscription`

- **Google BigQuery:**
  - Dataset: `latam_dataset`
  - Tabla: `datos`

- **Google Cloud Run:**
  - API desplegada para exposiciÃ³n de datos.

---
## ğŸ”„ Parte 2: Aplicaciones y flujo CI/CD

### **2.1 API HTTP**

La API HTTP fue desarrollada utilizando **FastAPI** para garantizar un alto rendimiento y facilidad de implementaciÃ³n. Los endpoints principales permiten la exposiciÃ³n de datos almacenados en la base de datos.

#### **Endpoints implementados:**
- **GET `/health`:** 
  - Verifica la salud de la API y confirma que estÃ¡ operativa.
  - **Respuesta de ejemplo:**
    ```json
    {
      "status": "ok"
    }
    ```

- **GET `/datos`:**
  - Recupera los Ãºltimos registros almacenados en **Google BigQuery**.
  - **Respuesta de ejemplo:**
    ```json
    {
      "datos": [
        {
          "id": "789",
          "contenido": "Prueba automÃ¡tica",
          "timestamp": "2024-01-14T14:00:00+00:00"
        }
      ]
    }
    ```

**LÃ³gica implementada:**
- ConexiÃ³n directa con **Google BigQuery** utilizando su cliente oficial de Python.
- Queries optimizadas para limitar los resultados y garantizar baja latencia.

---

### **2.2 Deployar API HTTP mediante CI/CD**

El despliegue de la API HTTP se realiza utilizando un flujo de **CI/CD con GitHub Actions**. Este pipeline automatiza los pasos de construcciÃ³n, prueba y despliegue en **Google Cloud Run**.

#### **2.2.1 Flujo del pipeline CI/CD:**
1. **Build Docker Image:**
   - ConstrucciÃ³n de la imagen Docker de la API.
   - Push de la imagen a **Google Container Registry (GCR)**.

2. **Deploy con Terraform:**
   - Terraform aplica los cambios para actualizar la infraestructura, incluyendo el despliegue de la API en **Google Cloud Run**.

3. **Pruebas AutomÃ¡ticas:**
   - Pruebas de integraciÃ³n y carga para validar los endpoints `/health` y `/datos`.

##### **Archivo GitHub Actions (`.github/workflows/deploy.yml`):**


### ğŸ“¦ Evidencias
- Pipeline exitoso: Logs de GitHub Actions.
https://github.com/fernachbauer/latam-challenge/actions/runs/12771103343

---
### **2.3 (Opcional) Ingesta de datos desde Pub/Sub**

En esta etapa opcional, se implementÃ³ una lÃ³gica para procesar los mensajes recibidos en **Google Cloud Pub/Sub** y almacenarlos en la base de datos **Google BigQuery**.

#### **Objetivo:**
- Configurar una suscripciÃ³n a **Pub/Sub** que permita:
  1. Recibir mensajes publicados en el tÃ³pico `datos-topic`.
  2. Procesar y validar los datos.
  3. Almacenar los datos recibidos en la tabla `datos` del dataset `latam_dataset` en **BigQuery**.


#### **Flujo del procesamiento:**

1. **PublicaciÃ³n de datos:**
   - Los datos son enviados al tÃ³pico `datos-topic` mediante el cliente de **Pub/Sub**.

2. **Procesamiento de mensajes:**
   - Se configura la suscripciÃ³n `datos-subscription` para recibir los mensajes del tÃ³pico.
   - Los mensajes son procesados a travÃ©s de un **callback** que:
     - Decodifica y valida los datos.
     - Inserta los registros en la tabla de **BigQuery**.

3. **InserciÃ³n en BigQuery:**
   - Los datos se insertan en la tabla `datos`, aprovechando las capacidades de la API de BigQuery.

#### Ventajas de esta implementaciÃ³n:
- **Escalabilidad:** Pub/Sub maneja un alto volumen de mensajes.
- **Flexibilidad:** Los datos malformados pueden ser gestionados en el callback.
- **IntegraciÃ³n serverless:** La combinaciÃ³n de Pub/Sub y BigQuery simplifica la operaciÃ³n.

#### **Evidencias:**
Mensajes enviados a Pub/Sub:

gcloud pubsub topics publish datos-topic \
  --message '{"id":"123","contenido":"Mensaje de prueba","timestamp":"2025-01-14T12:00:00Z"}'

messageIds:
- '13531497031954997'
---
### **2.4 Diagrama de Arquitectura:**
- https://drive.google.com/file/d/1XTSk-mBAVAPTKsSG6DKX_VgDACknp_h8/view?usp=sharing

<div align="center">
    <img src="./arquitectura-api-latam.png" alt="Arquitectura de la SoluciÃ³n" width="800"/>
</div>


#### Infraestructura Propuesta

1. **Ingesta de Datos (Google Cloud Pub/Sub)**

Componentes:
- TÃ³pico (datos-topic): Recibe los datos enviados por los productores.
- SuscripciÃ³n (datos-subscription): Entrega los mensajes a los consumidores.

RazÃ³n: Desacopla productores y consumidores, garantiza entrega de mensajes y escala automÃ¡ticamente.

2. **Almacenamiento de Datos (Google BigQuery)**

Componentes:
- Dataset (latam_dataset): Contiene la tabla datos.
- Tabla (datos): Almacena los datos con un esquema analÃ­tico optimizado.

RazÃ³n: BigQuery es ideal para consultas rÃ¡pidas y escalables en grandes volÃºmenes de datos.

3. **ExposiciÃ³n de Datos (FastAPI en Google Cloud Run)**

Componentes:
- FastAPI: API con endpoints /health y /datos.
- Cloud Run: Despliega la API como un contenedor serverless.

RazÃ³n: Cloud Run escala automÃ¡ticamente segÃºn la demanda, optimizando costos y garantizando disponibilidad.

4. **CI/CD con GitHub Actions**

Pipeline:
- ConstrucciÃ³n y despliegue de contenedores Docker.
- Infraestructura gestionada con Terraform.
- Pruebas de integraciÃ³n y carga automÃ¡ticas.

RazÃ³n: AutomatizaciÃ³n completa para garantizar calidad y despliegues consistentes.

**Resumen del Proceso**
PublicaciÃ³n de Datos: Los datos ingresan a travÃ©s de Pub/Sub.
Procesamiento: Se validan y almacenan en BigQuery.
Consumo: Los datos se exponen mediante una API HTTP desplegada en Cloud Run.
AutomatizaciÃ³n: CI/CD gestiona el flujo completo desde cÃ³digo hasta producciÃ³n.

---

## ğŸ§ª **Parte 3: Pruebas de IntegraciÃ³n y Puntos CrÃ­ticos de Calidad**

### **3.1 Implementar en el flujo CI/CD un test de integraciÃ³n que verifique que la API efectivamente estÃ¡ exponiendo los datos de la base de datos. ArgumentaciÃ³n.**

- **DescripciÃ³n**: Se incluyÃ³ un test de integraciÃ³n que valida si el endpoint `/datos` retorna datos almacenados en BigQuery.
- **Objetivo**: Garantizar que la API HTTP funcione correctamente y exponga los datos como se espera.
- **ImplementaciÃ³n**: Los tests se ejecutan automÃ¡ticamente en el pipeline CI/CD, aprovechando GitHub Actions.
- **CÃ³digo de tests**: /latam-challenge/tests/integration/test_api.py
```python
  def test_health_endpoint():
    response = requests.get(f"{BASE_URL}/health")
    assert response.status_code == 200
    assert response.json() == {"status": "ok"}
    
def test_datos_endpoint():
    response = requests.get(f"{BASE_URL}/datos")
    assert response.status_code == 200
    assert "datos" in response.json()
```
- **JustificaciÃ³n:**
Este test asegura que la conexiÃ³n entre la API y BigQuery estÃ© configurada correctamente.
AdemÃ¡s, se verifica la estructura y contenido de la respuesta de la API.

https://github.com/fernachbauer/latam-challenge/actions/runs/12771103343/job/35597552427
Run pytest tests/

============================= test session starts ==============================
platform linux -- Python 3.9.21, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/runner/work/latam-challenge/latam-challenge
plugins: anyio-4.8.0
collected 2 items

tests/integration/test_api.py ..                                         [100%]

============================== 2 passed in 1.09s ===============================
---
### **3.2 Proponer otras pruebas de integraciÃ³n que validen que el sistema estÃ¡ funcionando correctamente y cÃ³mo se implementarÃ­an**

* **Test de Ingesta de Datos:**

**DescripciÃ³n:** Validar que los datos publicados en Pub/Sub se almacenen correctamente en BigQuery.

**ImplementaciÃ³n propuesta:**
- Simular la publicaciÃ³n de un mensaje en el tÃ³pico datos-topic.
- Confirmar que los datos aparezcan en la tabla datos de BigQuery.

* **Test de validaciÃ³n de mensajes:**

**DescripciÃ³n:** Verificar que los mensajes malformados enviados a Pub/Sub no se almacenen en BigQuery.

ImplementaciÃ³n propuesta:
- Enviar un mensaje con formato incorrecto al tÃ³pico.
- Asegurar que no aparece en la tabla datos.

* **Test de Endpoint /health con Dependencias Externas:**

**DescripciÃ³n:** Verificar que el endpoint /health no solo confirma la disponibilidad de la API, sino tambiÃ©n la conectividad con servicios externos como BigQuery.

**CÃ³digo Ejemplo:**
```python
def test_health_with_dependencies():
    response = requests.get(f"{BASE_URL}/health")
    assert response.status_code == 200
    assert response.json() == {"status": "ok"}
    
    # Validar conectividad con BigQuery
    try:
        query = "SELECT 1"
        query_job = bigquery_client.query(query)
        assert query_job.result()
    except Exception as e:
        assert False, f"Error de conectividad con BigQuery: {e}"
```
---
### 3.3. âš ï¸ Puntos CrÃ­ticos Identificados y Pruebas de Rendimiento

ğŸ“‰ **Posibles Puntos CrÃ­ticos:**

1. **Latencia en consultas a BigQuery:**  
   - Bajo cargas elevadas, las consultas a BigQuery presentan tiempos de respuesta variables.  
   - **Pruebas de carga** revelaron que la latencia promedio llegÃ³ hasta **~600ms**, superando el umbral Ã³ptimo.

2. **Errores de ingesta de datos por formatos invÃ¡lidos:**  
   - La ausencia de validaciÃ³n previa de los datos provenientes de **Pub/Sub** podrÃ­a derivar en errores al insertar registros en **BigQuery**.

3. **Sobrecarga de la API bajo alta demanda:**  
   - Las pruebas de carga mostraron que el endpoint `/datos` maneja correctamente mÃºltiples solicitudes **GET**, pero:
     - El sistema registrÃ³ un **11.98% de fallos** en intentos de inserciÃ³n (**POST**) por falta de implementaciÃ³n de dicho endpoint.  
     - La latencia aumentÃ³ progresivamente, alcanzando picos de hasta **6 segundos** bajo carga continua.  
     - La **saturaciÃ³n** de recursos limitÃ³ la capacidad de respuesta eficiente.

ğŸ“ˆ **Pruebas de Carga Implementadas:**

Se utilizÃ³ **Locust** para evaluar el rendimiento del sistema:

https://github.com/fernachbauer/latam-challenge/actions/runs/12771103343/job/35597552427

- **Usuarios concurrentes:** 50  
- **Tasa de incremento:** 10 usuarios/segundo  
- **DuraciÃ³n:** 2 minutos  

ğŸ“„ **Resultados clave:**

- âœ… **GET /datos:** RespondiÃ³ exitosamente con una latencia promedio aceptable de **400ms**.  
- âŒ **POST /datos:** El 100% de los intentos fallaron con error **405** (**Method Not Allowed**), evidenciando la **ausencia de un endpoint de ingesta directa**.  
- ğŸ“‰ **Latencia creciente:** Se detectÃ³ un crecimiento sostenido en los tiempos de respuesta, alcanzando **hasta 6 segundos** en momentos crÃ­ticos.

ğŸ” **Conclusiones:**

- La API soporta cargas moderadas para **lectura**, pero **no escala adecuadamente** bajo alta demanda sostenida.  
- La falta de un endpoint para inserciÃ³n directa limita el flujo de ingesta eficiente.  
- La latencia creciente podrÃ­a afectar la experiencia del usuario final y generar **timeouts** en sistemas integrados.

ğŸ’¡ **Acciones Correctivas:**

1. **Optimizar consultas a BigQuery:**  
   - Implementar **particiones** y **clustering** para mejorar el tiempo de respuesta.  

2. **Implementar validaciÃ³n de datos en la ingesta:**  
   - Prevenir fallos de inserciÃ³n mediante validaciones estrictas antes de enviar datos a BigQuery.

3. **Incorporar balanceo de carga y autoescalado:**  
   - Desplegar instancias adicionales de la API en **Cloud Run** bajo demanda.  
   - Configurar balanceo de carga para distribuir el trÃ¡fico eficientemente.  

4. **Agregar el endpoint POST /datos:**  
   - Permitir la ingesta directa de datos y evitar sobrecargar el flujo Pub/Sub.
---

## **3.4 Proponer cÃ³mo robustecer tÃ©cnicamente el sistema para compensar o solucionar dichos puntos crÃ­ticos**

### **OptimizaciÃ³n de Consultas en BigQuery:**

- Usar particiones y clustering para mejorar el rendimiento de las consultas.
- Limitar el nÃºmero de registros retornados.

### **ValidaciÃ³n de Mensajes en Pub/Sub:**

- Implementar lÃ³gica de validaciÃ³n en el suscriptor antes de insertar datos en BigQuery.
- Registrar errores en un sistema de monitoreo para su anÃ¡lisis.

### **Autoescalado y Balanceo de Carga:**

- Configurar Cloud Run para escalar automÃ¡ticamente con base en las mÃ©tricas de latencia o CPU.
- Utilizar un balanceador de carga para distribuir el trÃ¡fico de manera eficiente.

### **Monitoreo y Alertas:**

- Usar Google Cloud Monitoring para rastrear mÃ©tricas clave como:
    * Latencia en BigQuery.
    * Errores en Pub/Sub.
    * Tiempo de respuesta de la API.
- Configurar alertas basadas en lÃ­mites crÃ­ticos (p. ej., latencia > 1s).

---
## ğŸ“Š **Parte 4: MÃ©tricas y Monitoreo**

### **4.1 Proponer 3 mÃ©tricas crÃ­ticas para entender la salud y rendimiento del sistema end-to-end**

1. **ğŸš€ Tasa de Ingesta de Mensajes en Pub/Sub (Messages Published Rate)**  
   - **DescripciÃ³n:** NÃºmero de mensajes publicados por segundo/minuto en el tÃ³pico `datos-topic`.  
   - **Objetivo:** Detectar cuellos de botella en la ingesta de datos.  

2. **â±ï¸ Latencia de Respuesta de la API (API Response Latency)**  
   - **DescripciÃ³n:** Tiempo promedio de respuesta del endpoint `/datos` y `/health`.  
   - **Objetivo:** Identificar degradaciÃ³n en el rendimiento de la API bajo diferentes cargas.  

3. **âŒ Tasa de Errores de Ingesta (Error Rate in Data Ingestion)**  
   - **DescripciÃ³n:** Porcentaje de mensajes fallidos al ser insertados desde Pub/Sub a BigQuery.  
   - **Objetivo:** Detectar problemas en el procesamiento de datos y prevenir pÃ©rdida de informaciÃ³n.  

---

### **4.2 Proponer una herramienta de visualizaciÃ³n y describe textualmente quÃ© mÃ©tricas mostrarÃ­a**

**ğŸ” Herramienta Propuesta:** *Google Cloud Monitoring (Stackdriver)*

**ğŸ“Š MÃ©tricas a visualizar:**

- **Tasa de Ingesta de Mensajes (Pub/Sub):** GrÃ¡fica de lÃ­neas que muestra la cantidad de mensajes procesados por segundo.  
- **Latencia de Respuesta de la API:** Panel con tiempos de respuesta promedio, mÃ¡ximo y percentiles.  
- **Errores de BigQuery:** Conteo de errores de inserciÃ³n de datos, diferenciando entre errores transitorios y crÃ­ticos.  
- **Uso de Recursos (CPU/RAM):** Estado de utilizaciÃ³n de recursos de Cloud Run.  

**ğŸ“ˆ Â¿CÃ³mo ayuda esta informaciÃ³n?**  
- **Rendimiento:** Detectar cuellos de botella y optimizar recursos.  
- **Estabilidad:** Identificar picos de latencia y prevenir fallos en la API.  
- **Disponibilidad:** Asegurar la ingesta continua de datos y reacciÃ³n ante errores crÃ­ticos.  

---

## **4.3 ImplementaciÃ³n de la herramienta de monitoreo en la nube**

**ğŸ”§ Pasos de ImplementaciÃ³n:**

1. **Activar Cloud Monitoring:**  
   - Configurar Google Cloud Monitoring para recolectar mÃ©tricas de Pub/Sub, Cloud Run y BigQuery.  

2. **Configurar ExportaciÃ³n de Logs:**  
   - Exportar logs de errores y mÃ©tricas de Pub/Sub a Cloud Monitoring.  

3. **Crear Dashboards Personalizados:**  
   - DiseÃ±ar paneles interactivos con grÃ¡ficos de lÃ­neas, indicadores y tablas.  

4. **Configurar Alertas AutomÃ¡ticas:**  
   - Definir umbrales para alertas crÃ­ticas (e.g., latencia > 1s, error rate > 5%).  

---

## **4.4 Escalamiento de la soluciÃ³n a 50 sistemas similares**

**ğŸ”„ Cambios en la VisualizaciÃ³n:**

Se debe tener una mirada mas general de los sistemas, abordando KPI agrupados y mÃ©tricas que permitan gestionar mayor volÃºmen de instancias o aplicaciones con menor esfuerzo. Para ello, hay que alinear las necesidades tÃ©cnicas de la continuidad operacional con la vista estratÃ©gica de la compaÃ±ia, prioridades de ingesta para procesos crÃ­ticos, calendario de jobs, se deben clasificar los tipos de procesos operacionales a lo largo de todo el ciclo de vida de los datos.

Dado esto se hace necesario crear un paneles tÃ¡cticos y estratÃ©gicos para poder despejar la informaciÃ³n y agruparla en sintonÃ­a con las decisiones que se deben tomar a nivel de recursos en la nube y el presupuesto asignado para los distintos recursos cloud.

Entonces, para administrar **50 sistemas similares** que exponen APIs y manejan flujos de datos crÃ­ticos, es esencial construir **paneles de monitoreo** eficientes que permitan supervisar tanto la **operaciÃ³n tÃ©cnica** como la **estrategia de negocio**. Esto implica usar visualizaciones e indicadores especÃ­ficos que faciliten la toma de decisiones. AquÃ­ te propongo una estructura de paneles con mÃ©tricas e indicadores clave:

Es por ello que se proponen algunas formas de segmentar recursos:

- **SegmentaciÃ³n por Proyecto/Sistema:** Agrupar mÃ©tricas por cada instancia del sistema.  
- **Dashboard Global:** Vista consolidada para todas las instancias, destacando los sistemas crÃ­ticos.  
- **MÃ©tricas Agregadas:**  
  - Tasa de ingesta global vs. individual. (Fallos, Incidenciasm etc)  
  - Latencia promedio global.  
  - Comparativa de errores entre sistemas.
  - Agrupaciones customizadas que aporten informaciÃ³n estratÃ©gica para un KPI de la compaÃ±ia. 

**ğŸ”“ Nuevas MÃ©tricas Desbloqueadas:**

## ğŸ“Š **Panel EstratÃ©gico de Alto Nivel (Executive Dashboard)**

**Objetivo:** Proveer una visiÃ³n global y simplificada del estado de los sistemas, alineada con los objetivos estratÃ©gicos de la empresa.

### ğŸ”‘ **Indicadores Clave (KPIs):**

- **Disponibilidad General (%):** Uptime consolidado de todos los sistemas.  
- **Tiempo Promedio de Respuesta (ms):** Latencia media de todas las APIs.  
- **Errores CrÃ­ticos (5xx) Globales:** NÃºmero de fallas crÃ­ticas por entorno.  
- **Uso de Recursos por Sistema:** CPU, memoria y almacenamiento usados.  
- **Costo de OperaciÃ³n ($):** Gasto mensual por servicio (APIs, Pub/Sub, BigQuery).  
- **Prioridad de Procesos CrÃ­ticos:** Estado de los sistemas clasificados por prioridad.

### ğŸ“Š **Visualizaciones:**

- **Heatmaps:** Mapas de calor para ver disponibilidad por regiÃ³n o servicio.  
- **GrÃ¡ficos de LÃ­neas:** EvoluciÃ³n del uso de recursos (CPU/RAM) en el tiempo.  
- **GrÃ¡ficos de Barras:** ComparaciÃ³n de costos operativos por sistema.  
- **SemÃ¡foros de Estado:** Indicadores visuales (verde, amarillo, rojo) para servicios crÃ­ticos.

---

## âš™ï¸ **Panel TÃ©cnico Operacional (Ops Dashboard)**

**Objetivo:** Supervisar el rendimiento y la salud operativa de cada API y su infraestructura.

### ğŸ”‘ **Indicadores Clave (KPIs):**

- **Latencia por API:** Tiempo de respuesta segregado por endpoint.  
- **Errores 4xx/5xx:** Tasa de errores de cliente y servidor.  
- **Tasa de Ã‰xito de Ingesta (Pub/Sub):** % de mensajes procesados correctamente.  
- **Backlogs de Pub/Sub:** Mensajes pendientes de procesar por suscripciÃ³n.  
- **Uso de BigQuery:** Consultas por segundo y tiempos de ejecuciÃ³n.

### ğŸ“Š **Visualizaciones:**

- **GrÃ¡ficos de LÃ­neas por Servicio:** Latencia, errores y trÃ¡fico por API.  
- **Histograma de Latencia:** DistribuciÃ³n de tiempos de respuesta.  
- **Panel de MÃ©tricas de Pub/Sub:** PublicaciÃ³n y procesamiento de mensajes.  
- **Alertas en Tiempo Real:** Panel con logs y alertas activas.

---

## ğŸš€ **Panel de Escalabilidad y Costos (Scaling & Cost Dashboard)**

**Objetivo:** Optimizar el uso de recursos y controlar costos ante el crecimiento de los sistemas.

### ğŸ”‘ **Indicadores Clave (KPIs):**

- **Uso de Autoescalado:** Niveles de escalamiento de Cloud Run.  
- **Costo por API:** Desglose de costos de operaciÃ³n por servicio.  
- **Costo por RegiÃ³n/Entorno:** Gastos segÃºn ubicaciÃ³n geogrÃ¡fica.  
- **Capacidad Reservada vs. Uso Real:** Eficiencia de recursos.

### ğŸ“Š **Visualizaciones:**

- **Stacked Bar Charts:** Costos por componente (API, almacenamiento, red).  
- **Heatmap de Uso de Recursos:** Uso de CPU y memoria en Cloud Run.  
- **GrÃ¡ficos de LÃ­neas:** Tendencia de costos vs. trÃ¡fico de usuarios.

---

## ğŸ›¡ï¸ **Panel de Seguridad y Cumplimiento (Security Dashboard)**

**Objetivo:** Garantizar la seguridad de los sistemas y el cumplimiento normativo.

### ğŸ”‘ **Indicadores Clave (KPIs):**

- **Intentos de Acceso Fallidos:** NÃºmero de intentos de acceso no autorizados.  
- **Errores de AutenticaciÃ³n:** Fallos de autenticaciÃ³n de usuarios/API Keys.  
- **Permisos y Roles Inadecuados:** Cambios inusuales en roles de IAM.  
- **Eventos de Seguridad:** Logs de incidentes de seguridad.

### ğŸ“Š **Visualizaciones:**

- **Tablas de AuditorÃ­a:** Accesos por usuario/servicio.  
- **GrÃ¡ficos de Radar:** Comparativa de riesgos por sistema.  
- **Timeline de Incidentes:** CronologÃ­a de eventos de seguridad.

---

## ğŸ› ï¸ **Herramientas para Implementar los Dashboards**

- **Google Cloud Monitoring (Stackdriver):** Para monitorear servicios de Google Cloud.  
- **Grafana:** VisualizaciÃ³n avanzada de mÃ©tricas tÃ©cnicas.  
- **BigQuery + Looker Studio:** AnÃ¡lisis de grandes volÃºmenes de datos.  
- **Prometheus:** RecolecciÃ³n de mÃ©tricas a nivel de infraestructura.  
- **PagerDuty / Opsgenie:** GestiÃ³n de alertas y respuesta a incidentes.

---

## ğŸ“ˆ **MÃ©tricas Avanzadas para Escalamiento**

- **Tasa de Peticiones Concurrentes:** RelaciÃ³n entre trÃ¡fico entrante y capacidad de respuesta.  
- **Colas de Mensajes (Pub/Sub):** DetecciÃ³n de cuellos de botella en la ingesta.  
- **Tasa de Fallos Transitorios:** Errores intermitentes que podrÃ­an ser mitigados con reintentos.  
- **Elasticidad de Autoescalado:** Capacidad del sistema de escalar de forma eficiente.  
- **Costo Eficiencia:** Costo por transacciÃ³n o por volumen de datos procesado.

---

## ğŸ”‘ **ConclusiÃ³n**

Para escalar a **50 sistemas similares**, es fundamental implementar dashboards claros y jerarquizados, donde se visualicen mÃ©tricas estratÃ©gicas, operacionales, de costos y de seguridad. Estos paneles permitirÃ¡n tomar decisiones informadas, optimizar recursos y garantizar la **resiliencia** y **escalabilidad** del sistema.

---

## **4.5 Dificultades o limitaciones en observabilidad por problemas de escalabilidad**

**âš ï¸ Riesgos Potenciales:**

1. **Sobrecarga de Monitoreo:**  
   - Aumento en costos por almacenamiento de logs y mÃ©tricas.  
   - SaturaciÃ³n de dashboards con exceso de datos.  

2. **Falsos Positivos en Alertas:**  
   - Umbrales mal configurados pueden generar alertas innecesarias.  

3. **FragmentaciÃ³n de InformaciÃ³n:**  
   - Dificultad para correlacionar mÃ©tricas si no se estandariza la recolecciÃ³n de datos.  

**ğŸ› ï¸ Propuestas de MitigaciÃ³n:**  
- **Uso de mÃ©tricas agregadas:** Consolidar datos para obtener insights globales.  
- **Optimizar reglas de alertas:** Definir umbrales dinÃ¡micos ajustados al contexto.  
- **Automatizar escalamiento:** Implementar autoescalado basado en mÃ©tricas crÃ­ticas.

---

# âš ï¸ Parte 5: Alertas y SRE (Opcional)

## 5.1 ğŸ”” **DefiniciÃ³n de Reglas y Umbrales de Alerta**

Las siguientes reglas de alerta estÃ¡n diseÃ±adas para anticipar problemas crÃ­ticos en el sistema y mantener su rendimiento Ã³ptimo.

### ğŸš¨ **Alertas CrÃ­ticas**

| **MÃ©trica**                                  | **Umbral CrÃ­tico**                             | **Impacto**                                            | **AcciÃ³n Correctiva**                                   |
|----------------------------------------------|------------------------------------------------|-------------------------------------------------------|--------------------------------------------------------|
| **Disponibilidad de la API HTTP**            | < 99.9% de uptime mensual                      | PÃ©rdida de disponibilidad del servicio.                | Escalar al equipo de infraestructura.                   |
| **Latencia de Respuesta de la API**          | > 500ms sostenido por mÃ¡s de 5 minutos         | DegradaciÃ³n de la experiencia del usuario.             | Activar autoescalado en Cloud Run.                     |
| **Errores 5xx en la API HTTP**              | > 2% del total de peticiones en 10 minutos     | Fallos en la infraestructura o backend.               | Inspeccionar logs y aplicar rollback si es necesario.  |
| **Tasa de Mensajes No Procesados (Pub/Sub)** | > 5% durante 15 minutos                        | Riesgo de pÃ©rdida o retraso de datos.                 | Incrementar capacidad de procesamiento.                |
| **Uso de CPU/Memoria**                      | CPU > 85% o RAM > 80% por mÃ¡s de 10 minutos    | SaturaciÃ³n de instancias y posible caÃ­da del servicio.| Revisar autoescalado y optimizar recursos.             |

### âš ï¸ **Alertas Moderadas**

| **MÃ©trica**                                | **Umbral**                              | **Impacto**                                 | **AcciÃ³n Correctiva**                           |
|--------------------------------------------|----------------------------------------|--------------------------------------------|------------------------------------------------|
| **Errores 4xx en API HTTP**                | > 10% del total de peticiones           | Problemas con solicitudes incorrectas.     | Revisar uso de la API y mejorar documentaciÃ³n. |
| **Costos de OperaciÃ³n**                    | > 20% del presupuesto mensual proyectado | Riesgo financiero por sobrecostos.         | Revisar configuraciones y recursos asignados.  |
| **Backlog en Pub/Sub**                     | Crecimiento constante sin estabilizarse | Cuello de botella en la ingesta de datos.  | Aumentar paralelismo en la suscripciÃ³n.        |

---

## 5.2 ğŸ“ **DefiniciÃ³n de SLIs/SLOs**

### ğŸ” **SLIs (Service Level Indicators)**

| **SLI**                                     | **DescripciÃ³n**                                          | **Umbral CrÃ­tico**                       | **Herramienta de Monitoreo**                  |
|---------------------------------------------|----------------------------------------------------------|-----------------------------------------|----------------------------------------------|
| **Disponibilidad de la API HTTP**           | % de tiempo que la API responde correctamente (2xx).     | < 99.9% mensual                         | Google Cloud Monitoring (Stackdriver).       |
| **Latencia de Respuesta de la API HTTP**    | Tiempo promedio de respuesta.                            | > 500ms sostenido por 5 minutos.        | Stackdriver, mÃ©tricas de Cloud Run.         |
| **Tasa de Error (Error Rate)**              | % de respuestas 5xx sobre el total de peticiones.        | > 2% sostenido en 10 minutos.          | Logs y mÃ©tricas de Cloud Run.              |
| **Procesamiento en Pub/Sub**                | % de mensajes procesados exitosamente.                   | < 95% de procesamiento en 15 minutos.  | MÃ©tricas de Pub/Sub.                      |

### ğŸ¯ **SLOs (Service Level Objectives)**

| **SLO**                                     | **Objetivo**                                   | **JustificaciÃ³n**                                  |
|---------------------------------------------|------------------------------------------------|----------------------------------------------------|
| **Disponibilidad de la API HTTP**           | â‰¥ 99.9% mensual                                | Minimizar interrupciones y asegurar continuidad.   |
| **Latencia de Respuesta**                   | â‰¤ 500ms en el 95% de las peticiones.           | Garantizar experiencia de usuario fluida.          |
| **Tasa de Error**                           | â‰¤ 1% de errores 5xx por semana.               | Mantener confiabilidad y disponibilidad.           |
| **Procesamiento en Pub/Sub**                | â‰¥ 98% de mensajes procesados sin error.        | Asegurar flujo continuo de ingesta de datos.       |

---

## ğŸ’¡ **Razonamiento de SelecciÃ³n de SLIs/SLOs**

| **RazÃ³n de SelecciÃ³n**                   | **MÃ©tricas Incluidas**                                  | **MÃ©tricas Excluidas**                             |
|-----------------------------------------|--------------------------------------------------------|---------------------------------------------------|
| **Impacto directo en el usuario**       | Disponibilidad, Latencia, Errores 5xx.                 | Uso de disco y mÃ©tricas internas no visibles.     |
| **Continuidad del negocio**             | Tasa de procesamiento en Pub/Sub.                      | MÃ©tricas de trÃ¡fico interno.                      |
| **Balance entre rigor y flexibilidad**  | Umbrales adaptados al impacto crÃ­tico.                 | MÃ©tricas que no afectan al usuario final.         |

---

## ğŸ“Š **Resumen de la Estrategia de Alertas y SRE**

- **ğŸš¨ DetecciÃ³n proactiva:** Las alertas y umbrales estÃ¡n diseÃ±ados para actuar antes de que los problemas impacten a los usuarios.  
- **ğŸ“ˆ MÃ©tricas alineadas al negocio:** Los SLIs/SLOs garantizan el cumplimiento de objetivos estratÃ©gicos.  
- **âš™ï¸ Resiliencia operativa:** La configuraciÃ³n de alertas y escalado automÃ¡tico permite mantener un servicio confiable y de alto rendimiento.  

---

## 5.3 ğŸ¯ **SLIs y SLOs Propuestos**

Los **SLIs (Service Level Indicators)** y **SLOs (Service Level Objectives)** definen los compromisos de rendimiento y disponibilidad del sistema.

### ğŸ“ **Service Level Indicators (SLIs)**

- **Latencia de la API:** Tiempo de respuesta promedio de la API.  
- **Tasa de errores:** Porcentaje de solicitudes fallidas (errores 5xx).  
- **Disponibilidad del sistema:** Porcentaje de tiempo que la API estÃ¡ disponible.

### âœ… **Service Level Objectives (SLOs)**

| **Indicador**           | **Objetivo (SLO)**                      |
|------------------------|----------------------------------------|
| **Disponibilidad**      | â‰¥ **99.9%** de tiempo activo mensual.  |
| **Latencia de la API**  | â‰¤ **500 ms** en el 95% de las peticiones. |
| **Tasa de errores**     | â‰¤ **1%** de respuestas con error.      |

---

## 5.4 ğŸ›¡ï¸ **Propuestas para Mejorar la Resiliencia del Sistema**

Para mitigar posibles riesgos y mejorar la disponibilidad, se proponen las siguientes medidas:

### ğŸ“ˆ **Escalabilidad AutomÃ¡tica (Auto Scaling):**  
- Configurar **Cloud Run** para autoescalar en funciÃ³n de la demanda.  
- Ajustar particionamiento y clustering en **BigQuery** para optimizar las consultas.

### ğŸ”„ **Implementar Retrys Exponenciales:**  
- Configurar reintentos automÃ¡ticos en **Pub/Sub** para manejar fallos transitorios.  
- Incorporar circuit breakers en la API.

### ğŸ› ï¸ **ValidaciÃ³n de Datos:**  
- Validar la estructura de los mensajes antes de insertarlos en **BigQuery**.

### ğŸ·ï¸ **Etiquetado de Recursos:**  
- Uso de etiquetas para segmentar recursos crÃ­ticos y aplicar reglas especÃ­ficas de monitoreo.

---

## 5.5 ğŸš§ **Dificultades y Limitaciones de Observabilidad**

Si no se abordan adecuadamente los desafÃ­os de escalabilidad y monitoreo, podrÃ­an surgir los siguientes problemas:

- **Alertas Falsas Positivas:** Alertas mal configuradas que saturan los canales de notificaciÃ³n.  
- **Falta de Visibilidad:** MÃ©tricas incompletas o dispersas dificultan la identificaciÃ³n de cuellos de botella.  
- **Escalabilidad del Monitoreo:** Al escalar a mÃºltiples instancias, puede volverse complejo gestionar las mÃ©tricas y alertas.  
- **Costos Elevados:** Exceso de monitoreo puede generar costos innecesarios si no se optimizan los recursos.

**ğŸ”‘ SoluciÃ³n:**  
- Configurar correctamente los umbrales de alertas.  
- Implementar dashboards agregados por ambiente.  
- Uso eficiente de los recursos para balancear costos y rendimiento.

---

## âœˆï¸ **ConclusiÃ³n**

La implementaciÃ³n de alertas proactivas, junto con mÃ©tricas clave y objetivos claros de disponibilidad y rendimiento, garantizan la **resiliencia**, **escalabilidad** y **fiabilidad** del sistema. Las acciones correctivas y preventivas propuestas mitigan los posibles riesgos, asegurando la continuidad operativa.

---

```
        __|__
--@--@--(_)--@--@--
```

---


# ğŸ‰ **Â¡Gracias por la Oportunidad!** ğŸ™Œ

Quiero expresar mi mÃ¡s sincero agradecimiento por la oportunidad de participar en el **LATAM Airlines DevSecOps/SRE Challenge**. ğŸŒâœˆï¸

Muchas gracias por recibir mi desafÃ­o, en el van muchas horas de esfuerzo y motivaciÃ³n por la oportunidad de formar parte del equipo.

Este proyecto ha sido una experiencia increÃ­ble, en la que he invertido muchas horas llenas de **motivaciÃ³n**, **aprendizaje** e **ilusiÃ³n**. Cada lÃ­nea de cÃ³digo, cada despliegue y cada prueba fueron pensados con el compromiso de entregar una soluciÃ³n robusta ,a la altura del desafÃ­o y la compaÃ±ia. ğŸ’» âœˆï¸âœˆï¸âœˆï¸âœˆï¸âœˆï¸âœˆï¸

Â¡Estoy emocionado por todo lo aprendido y por seguir creciendo en el mundo de la tecnologÃ­a! ğŸŒ

---

## ğŸ”§âœ¨âœˆï¸ **Â¡Nos vemos en las nubes!** â˜ï¸
--- 
ğŸ“‚ Repositorio
ğŸ”— https://github.com/fernachbauer/latam-challenge

ğŸ“§ Contacto
Nombre: Fernando Nachbauer R
Correo: fernachbauer@gmail.com
---
